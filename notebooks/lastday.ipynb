{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8e46ce-abc7-40c3-8378-982119d0dd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from utils.utils import load_cfg_and_ckpt_path\n",
    "from pathlib import Path\n",
    "from streaming.b3_streamer_v4 import MultiStreamer\n",
    "from dataloader.datamodule_bravo_multi import ECoGDataModule\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n",
    "import tqdm, string, re\n",
    "import os\n",
    "from streaming.b3_streamer_v4 import build_dualstreamer_from_file\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a34f04c-d9df-43bf-a8ea-43170b7a0a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "asr = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
    "asr = asr.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4035b34-9176-4fef-8208-5643f9a74bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_FILE_PATH = '/data/common/b3_paper/packaged_data/streaming_paper_data/tm1k_mimed_slow/tm1k_very_recent_test_b3_tm1k.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e5f4fc6-3efd-436b-ad77-1721fd3ef0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_config = {'root_dir': '/data/common/b3_paper/packaged_data/streaming_paper_data',\n",
    "              'hb_dir': '/data/cheoljun/b3_audio_scale-2/unit_label/hubert-l6_km100/',\n",
    "              'textidx_file': None,\n",
    "               'text_label_file': '/data/cheoljun/b3_misc/audio_info/tm1k_text_20240108.txt',\n",
    "               'metainfo_path':'/data/common/b3_paper/packaged_data/streaming_paper_data/tm1k_mimed_slow/tm1k_streamer_paper_final_session_dataframe.csv',\n",
    "               'paradigm':   'tm1k_mimed_slow',\n",
    "               'hb_paradigm': 'tm1k',\n",
    "               'batch_size': 24,\n",
    "               'num_workers': 1,\n",
    "               'include_ecog': True,\n",
    "               'include_unit': True,\n",
    "               'include_phoneme': True,\n",
    "               'load_list': ['ecog'],\n",
    "               'text_list': ['unit', 'phoneme'],\n",
    "               'relabeled':True,\n",
    "  'train_files':'/data/common/b3_paper/packaged_data/streaming_paper_data/tm1k_mimed_slow/cleaned_train_b3_tm1k.txt',\n",
    "  'test_files': TEST_FILE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccd50a-9d6d-42ff-9dcd-f395494bf91f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@ 0 blocks are overlapping in train and val! @@@@@@@\n"
     ]
    }
   ],
   "source": [
    "datamodule = ECoGDataModule(**data_config)\n",
    "test_loader = datamodule.test_dataloader()\n",
    "test_loader.dataset.transform=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f33ea-fd11-475e-b73b-d23fc2037dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wer(gt_text,pred_text):\n",
    "    def tokenize(text):\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        text = regex.sub('', text.lower())\n",
    "        return [t for t in text.split(' ') if t != '']\n",
    "    pred_text_tokens = tokenize(pred_text)\n",
    "    gt_text_tokens = tokenize(gt_text)\n",
    "    \n",
    "    return torchaudio.functional.edit_distance(pred_text_tokens,gt_text_tokens) /len(gt_text_tokens)\n",
    "\n",
    "def flatten_list(data):\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.extend(i)\n",
    "    return result\n",
    "\n",
    "def get_asr(wavs):\n",
    "    with torch.no_grad():\n",
    "        input_features = processor(wavs, sampling_rate=16000, return_tensors=\"pt\").input_features.to('cuda')\n",
    "        predicted_ids = asr.generate(input_features)\n",
    "        synth_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return synth_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1963929-5d7d-4e43-b078-171b9680e005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_names = ['tm1k_restricted', 'tm1k_super_restricted','tm1k_recent_restricted',\n",
    "                'tm1k_recent_super_restricted','tm1k_very_recent',\n",
    "                'textonly_tm1k_restricted', 'textonly_tm1k_super_restricted','textonly_tm1k_recent_restricted',\n",
    "                'textonly_tm1k_recent_super_restricted','textonly_tm1k_very_recent',]\n",
    "for config_name in config_names:\n",
    "    multistreamer, configs = build_dualstreamer_from_file(f'{config_name}.yaml', base_dir='/data/common/b3_model_package_20240125')               \n",
    "for config_name in config_names:\n",
    "    multistreamer, configs = build_dualstreamer_from_file(f'{config_name}.yaml', base_dir='/data/common/b3_model_package_20240125')\n",
    "    text_wer_results=[]\n",
    "    synth_wer_results=[]\n",
    "    nonexpand_synth_wer_results=[]\n",
    "    gt_synth_wer_results=[]\n",
    "    first_text_emit_times = []\n",
    "    buffer_size = configs['buffer_size']\n",
    "    wavs_all = []\n",
    "    for di in tqdm.tqdm(range(len(test_loader.dataset))):\n",
    "        x = test_loader.dataset.__getitem__(di)\n",
    "        input_all = x['ecog']\n",
    "\n",
    "        wavs = []\n",
    "        multistreamer.clear_cache()\n",
    "        first_text_emit_time = -1\n",
    "        for i in range(0,int(len(input_all)//buffer_size*buffer_size),buffer_size):\n",
    "            ecog = input_all[i:i+buffer_size]\n",
    "            outputs = multistreamer(ecog)\n",
    "            wav= outputs['wav']\n",
    "            if 'text' in outputs:\n",
    "                text=outputs['text']\n",
    "            else:\n",
    "                text = ''\n",
    "            if len(text)>0 and first_text_emit_time <0:\n",
    "                first_text_emit_time = (i+1)*5 # in ms unit\n",
    "            #print(f'PRED[{int(1/200*i*1000):03d}ms]-', f'{abs(wav).mean():.02f}', text)\n",
    "            wavs.append(wav)\n",
    "        first_text_emit_times.append(first_text_emit_time)\n",
    "        # clearing buffer\n",
    "        for i in range(100):\n",
    "            wavs.append(multistreamer(None)['wav'])\n",
    "        wavs = np.concatenate(wavs)\n",
    "        wavs_all.append(wavs)\n",
    "        synth_text = get_asr(wavs)\n",
    "\n",
    "        #nonexpand_wavs = multistreamer.streamers[0].vocoder.synthesize_v2(torch.tensor(flatten_list(multistreamer.streamers[0].unit_history)), alpha = 2.0, min_dur = 1)\n",
    "        #nonexpand_synth_text =  get_asr(nonexpand_wavs)\n",
    "\n",
    "        #gt_synth = multistreamer.streamers[0].vocoder.synthesize_v2(torch.tensor(np.array(x['text']['unit'].split(' ')).astype(int)), skip_duration=True)\n",
    "        #gt_synth_text = get_asr(gt_synth)\n",
    "        gt_text = x['text']['phoneme']\n",
    "        text_wer = get_wer(gt_text,text)\n",
    "        synth_wer = get_wer(gt_text, synth_text)\n",
    "        #nonexpand_synth_wer = get_wer(gt_text, nonexpand_synth_text)\n",
    "        #gt_synth_wer = get_wer(gt_text, gt_synth_text)\n",
    "        synth_wer_results.append(synth_wer)\n",
    "        #nonexpand_synth_wer_results.append(nonexpand_synth_wer)\n",
    "        #gt_synth_wer_results.append(gt_synth_wer)\n",
    "        text_wer_results.append(text_wer)\n",
    "\n",
    "    model_name = Path(configs['rnnt_ckpt_path']).stem\n",
    "    wers = [text_wer_results, synth_wer_results]#, nonexpand_synth_wer_results, gt_synth_wer_results]\n",
    "    labels = ['BPE','HB100']#,'HB100\\nNon-realtime','Ground truth\\nHB100' ]\n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(5, 4))\n",
    "\n",
    "    # rectangular box plot\n",
    "    bplot1 = ax1.boxplot(wers,\n",
    "                         vert=True,  # vertical box alignment\n",
    "                         patch_artist=True,  # fill with color\n",
    "                         labels=labels)  # will be used to label x-ticks\n",
    "    ax1.set_ylabel('WER')\n",
    "    for i, wer in enumerate(wers):\n",
    "        wer=[w for w in wer if w <20]\n",
    "        ax1.text(i+.85,1.6, f'{np.mean(wer):.02f}')\n",
    "    ax1.set_ylim(0,1.7)\n",
    "    ax1.set_title(f'{config_name}',fontsize=12)\n",
    "    \n",
    "    for wer, label in zip(wers, labels):\n",
    "        l_ = label.replace('\\n', ' ')\n",
    "        wer=[w for w in wer if w <20]\n",
    "        print(f'[{config_name}] WER of {l_} - {np.mean(wer):.02f}')\n",
    "    print(f'[{config_name}] First text emit time - {np.mean(first_text_emit_times):.01f}ms')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3708e-5922-4564-89fa-edf5ca1778d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnnt",
   "language": "python",
   "name": "rnnt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
